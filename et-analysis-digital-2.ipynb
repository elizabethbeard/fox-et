{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone Conditions by Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to try sorting out the phone conditions out by individual ad\n",
    "# libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import av\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.interpolate import interp1d\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Volumes/T7 Shield/fox/et/sub-095_phone-a-87a1bd3d',\n",
       " '/Volumes/T7 Shield/fox/et/sub-096_phone-b-3611b179',\n",
       " '/Volumes/T7 Shield/fox/et/sub-097_phone-b-ebeeac21',\n",
       " '/Volumes/T7 Shield/fox/et/sub-098_phone-b-29fbdee7',\n",
       " '/Volumes/T7 Shield/fox/et/sub-099_phone-b-55ff7c53',\n",
       " '/Volumes/T7 Shield/fox/et/sub-100_phone-b-66bd4dcc',\n",
       " '/Volumes/T7 Shield/fox/et/sub-104_phone-b-4cafddda',\n",
       " '/Volumes/T7 Shield/fox/et/sub-105_phone-a-448ad877',\n",
       " '/Volumes/T7 Shield/fox/et/sub-106_phone-b-1d313c9f',\n",
       " '/Volumes/T7 Shield/fox/et/sub-107_phone-a-6661b0ea',\n",
       " '/Volumes/T7 Shield/fox/et/sub-109_phone-a-463e65ac',\n",
       " '/Volumes/T7 Shield/fox/et/sub-110_phone-b-a96e2678',\n",
       " '/Volumes/T7 Shield/fox/et/sub-111_phone-a-f197f0ab',\n",
       " '/Volumes/T7 Shield/fox/et/sub-115_phone-a-f60825c4',\n",
       " '/Volumes/T7 Shield/fox/et/sub-116_phone-a-d9c05810',\n",
       " '/Volumes/T7 Shield/fox/et/sub-117_phone-b-4d98cbb8',\n",
       " '/Volumes/T7 Shield/fox/et/sub-119_phone-b-b2c5cdc2',\n",
       " '/Volumes/T7 Shield/fox/et/sub-158_phone-a-0ef92d52',\n",
       " '/Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541',\n",
       " '/Volumes/T7 Shield/fox/et/sub-162_phone-b-873e8eb1',\n",
       " '/Volumes/T7 Shield/fox/et/sub-163_phone-a-1e53b8a8',\n",
       " '/Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc',\n",
       " '/Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d',\n",
       " '/Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945',\n",
       " '/Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e',\n",
       " '/Volumes/T7 Shield/fox/et/sub-171_phone-b-yt-2da75edf',\n",
       " '/Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49',\n",
       " '/Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e',\n",
       " '/Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290',\n",
       " '/Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61',\n",
       " '/Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14',\n",
       " '/Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e',\n",
       " '/Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477',\n",
       " '/Volumes/T7 Shield/fox/et/sub-187_phone-a-d4aaafdf',\n",
       " '/Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f',\n",
       " '/Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7',\n",
       " '/Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6',\n",
       " '/Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42',\n",
       " '/Volumes/T7 Shield/fox/et/sub-192_phone-b-8962a1bf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths and files for subj looping\n",
    "t7sheild = os.path.join('/Volumes/T7 Shield/fox')\n",
    "subj_folders = glob.glob(os.path.join('/Volumes/T7 Shield/fox/et','sub-[0-9][0-9][0-9]_phone-*'))\n",
    "subj_folders = [folder for folder in subj_folders if 'control' not in folder]\n",
    "subj_folders.sort()\n",
    "print(len(subj_folders))\n",
    "subj_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths and files for testing\n",
    "# locally\n",
    "# subj_folder = os.path.join('data', 'sub-053_tv-lego-63478be7')\n",
    "# video = os.path.join(subj_folder, 'a5ba9c89_0.0-813.692.mp4')\n",
    "# events = pd.read_csv(os.path.join(subj_folder,'events.csv'))\n",
    "# gaze = pd.read_csv(os.path.join(subj_folder,'gaze.csv'))\n",
    "# world_timestamps = pd.read_csv(os.path.join(subj_folder,'world_timestamps.csv'))\n",
    "# task_events = pd.read_csv('/Users/ebeard/Dropbox (Penn)/i3/foxmedia/data/tv_event/sub-053_lego/sub-053_task-events.csv')\n",
    "\n",
    "# on t7shield\n",
    "subj_folder = subj_folders[2]\n",
    "big_video = glob.glob(os.path.join(subj_folder, '*.mp4'))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(error_message, subj=None):\n",
    "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    log_filename = f\"error_log-phone-{today_date}.txt\"\n",
    "    with open(log_filename, \"a\") as log_file:\n",
    "        if subj:\n",
    "            log_file.write(f\"Subject: {subj}\\n\")\n",
    "        log_file.write(f\"Error: {error_message}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Individal Ads\n",
    "We don't need to align the pupil labs events because we've decided to hand-code instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "subj = subj_folder.split('/')[5].split('_')[0]\n",
    "cond = subj_folder.split('/')[5].split('-')[2]\n",
    "\n",
    "events = pd.read_csv(os.path.join(subj_folder,'events.csv'))\n",
    "world_timestamps = pd.read_csv(os.path.join(subj_folder,'world_timestamps.csv'))\n",
    "gaze = pd.read_csv(os.path.join(subj_folder,'gaze.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Ad Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_onsets_and_durations(events_df, world_timestamps_df):\n",
    "\n",
    "    # Ensure events_df and world_timestamps_df are DataFrames\n",
    "    if not isinstance(events_df, pd.DataFrame):\n",
    "        events_df = pd.DataFrame(events_df)\n",
    "\n",
    "    if not isinstance(world_timestamps_df, pd.DataFrame):\n",
    "        world_timestamps_df = pd.DataFrame(world_timestamps_df)\n",
    "        \n",
    "    # Filter events for start and end using regex to match a digit followed by .start or .end\n",
    "    start_events = events_df[events_df['name'].str.contains(r'\\d\\.start')]\n",
    "    end_events = events_df[events_df['name'].str.contains(r'\\d\\.end')]\n",
    "\n",
    "    # Ensure the events are sorted by timestamp\n",
    "    start_events = start_events.sort_values(by='timestamp [ns]').reset_index(drop=True)\n",
    "    end_events = end_events.sort_values(by='timestamp [ns]').reset_index(drop=True)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    event_ids = []\n",
    "    onsets = []\n",
    "    durations = []\n",
    "    nearest_timestamps = []\n",
    "\n",
    "    for i, __ in start_events.iterrows():\n",
    "        start_event = start_events.iloc[i]\n",
    "        end_event = end_events.iloc[i]\n",
    "\n",
    "        # Calculate onset and duration\n",
    "        onset = start_event['timestamp [ns]']\n",
    "        duration = end_event['timestamp [ns]'] - onset\n",
    "\n",
    "        # Find the nearest timestamp in world_timestamps\n",
    "        nearest_timestamp = world_timestamps_df.iloc[(world_timestamps_df['timestamp [ns]'] - onset).abs().argmin()]['timestamp [ns]']\n",
    "\n",
    "        # Append results to lists\n",
    "        event_ids.append(i + 1)\n",
    "        onsets.append(onset)\n",
    "        durations.append(duration)\n",
    "        nearest_timestamps.append(nearest_timestamp)\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'event_id': event_ids,\n",
    "        'onset_ns': onsets,\n",
    "        'duration_ns': durations,\n",
    "        'nearest_timestamp_ns': nearest_timestamps\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# aligned_events_df = create_event_onsets_and_durations(events, world_timestamps)\n",
    "# print(aligned_events_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "aligned_events_df = create_event_onsets_and_durations(events, world_timestamps)\n",
    "print(aligned_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adjusted_onsets_vs_world_timestamps(aligned_events_df, world_timestamps_df, save_dir):\n",
    "    # Extract values for plotting\n",
    "    adjusted_onsets = aligned_events_df['onset_ns']\n",
    "    world_timestamps = world_timestamps_df['timestamp [ns]']\n",
    "\n",
    "    # Calculate min and max of adjusted_onsets\n",
    "    min_onset = adjusted_onsets.min()\n",
    "    max_onset = adjusted_onsets.max()\n",
    "\n",
    "    # Define the range within 5 values of min and max\n",
    "    lower_bound = min_onset \n",
    "    upper_bound = max_onset \n",
    "\n",
    "    # Filter world_timestamps based on the defined range\n",
    "    filtered_world_timestamps = world_timestamps[(world_timestamps >= lower_bound) & (world_timestamps <= upper_bound)]\n",
    "\n",
    "    # print(len(adjusted_onsets))\n",
    "    # print(len(world_timestamps))\n",
    "    # print(len(filtered_world_timestamps))\n",
    "\n",
    "    # Plot adjusted onsets\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.scatter(adjusted_onsets, adjusted_onsets, color='red', label='Adjusted Onsets', marker='x')\n",
    "\n",
    "    # Plot world timestamps\n",
    "    plt.plot(filtered_world_timestamps, filtered_world_timestamps, label='World Timestamps', alpha=0.7)\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel('Timestamps (ns)')\n",
    "    plt.ylabel('')\n",
    "    plt.title('Adjusted Onsets vs. World Timestamps')\n",
    "    plt.legend()\n",
    "    \n",
    "    # # Save the plot\n",
    "    plot_path = os.path.join(save_dir, 'adjusted_onsets_vs_world_timestamps.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return plot_path\n",
    "\n",
    "# Plot the data\n",
    "#plot_adjusted_onsets_vs_world_timestamps(aligned_events_df_2, world_timestamps)\n",
    "\n",
    "# we'll want to update these to save the plots into the correct folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_adjusted_onsets_vs_world_timestamps(aligned_events_df, world_timestamps, subj_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nearest_timestamps(aligned_events_df, save_dir):\n",
    "    # Extract values for plotting\n",
    "    adjusted_onsets = aligned_events_df['onset_ns']\n",
    "    nearest_timestamps = aligned_events_df['nearest_timestamp_ns']\n",
    "    event_ids = aligned_events_df['event_id']\n",
    "\n",
    "    # Plot nearest timestamps\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.scatter(adjusted_onsets, event_ids, color='blue', label='Adjusted Onsets', marker='o', alpha=0.8)\n",
    "    plt.scatter(nearest_timestamps, event_ids, color='green', label='Nearest Timestamps', marker='x')\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel('Timestamps (ns)')\n",
    "    plt.ylabel('Event ID')\n",
    "    plt.title('Adjusted Onsets and Nearest Timestamps per Event')\n",
    "    plt.legend()\n",
    "    \n",
    "    # # Save the plot\n",
    "    plot_path = os.path.join(save_dir, 'adjusted_onsets_nearest_timestamps.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return plot_path\n",
    "\n",
    "# Plot the nearest timestamps\n",
    "# plot_nearest_timestamps(aligned_events_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nearest_timestamps(aligned_events_df, subj_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_durations(aligned_events_df, save_dir):\n",
    "    # Extract values for plotting\n",
    "    event_ids = aligned_events_df['event_id']\n",
    "    adjusted_onsets = aligned_events_df['onset_ns']\n",
    "    durations = aligned_events_df['duration_ns']\n",
    "\n",
    "    # Calculate end times based on durations\n",
    "    adjusted_ends = adjusted_onsets + durations\n",
    "\n",
    "    # Plot event durations\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, event_id in enumerate(event_ids):\n",
    "        plt.plot([adjusted_onsets[i], adjusted_ends[i]], [event_id, event_id], marker='|', label=f'Event {event_id}' if i == 0 else \"\")\n",
    "\n",
    "    # Labels and legend\n",
    "    plt.xlabel('Timestamps (ns)')\n",
    "    plt.ylabel('Event ID')\n",
    "    plt.title('Event Durations and Onsets')\n",
    "    plt.legend()\n",
    "    \n",
    "    # # Save the plot\n",
    "    plot_path = os.path.join(save_dir, 'event_durations_and_onsets.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return plot_path\n",
    "# Plot the event durations\n",
    "# plot_event_durations(aligned_events_df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_event_durations(aligned_events_df, subj_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code to handle aligned_events_df_2\n",
    "# Save frames from aligned events (for QC/testing)\n",
    "# find frame using pyav\n",
    "container = av.open(video)\n",
    "video_stream = container.streams.video[0]\n",
    "\n",
    "# save_dir = os.path.join(subj_folder, 'ads')\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for index, row in aligned.iterrows():\n",
    "\n",
    "    event_id = row['event_id']\n",
    "    nearest_timestamp_ns = row['nearest_timestamp_ns']\n",
    "    duration = row['duration_ns']\n",
    "\n",
    "    # Calculate the second timestamp\n",
    "    second_timestamp_ns = nearest_timestamp_ns + duration\n",
    "\n",
    "    # Find the frame numbers for both timestamps\n",
    "    frame_num_1 = np.searchsorted(world_timestamps['timestamp [ns]'], nearest_timestamp_ns)\n",
    "    frame_num_2 = np.searchsorted(world_timestamps['timestamp [ns]'], second_timestamp_ns)\n",
    "\n",
    "    frame_num_1 = int(frame_num_1)\n",
    "    frame_num_2 = int(frame_num_2)\n",
    "\n",
    "    # Save both frames\n",
    "    for frame_num in [frame_num_1, frame_num_2]:\n",
    "        timestamp = frame_num * video_stream.time_base\n",
    "        container.seek(int(timestamp * av.time_base), stream=video_stream)\n",
    "\n",
    "        fc = 0\n",
    "        for frame in container.decode(video=0):\n",
    "            if fc == frame_num:\n",
    "                frame = frame.to_ndarray(format=\"bgr24\")\n",
    "                save_path = os.path.join(save_dir, f'event-{int(event_id)}_{frame_num}.jpg')\n",
    "                cv2.imwrite(save_path, frame)\n",
    "                break\n",
    "                \n",
    "            fc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Pupil Labs Videos and Subset Gaze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_segments_phone(video, world_timestamps, events_df, gaze, save_dir):\n",
    "    # Check the duration of the input video using FFmpeg\n",
    "    def get_video_duration(video_path):\n",
    "        ffmpeg_command = [\n",
    "            'ffprobe',\n",
    "            '-v', 'error',\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-of', 'default=noprint_wrappers=1:nokey=1',\n",
    "            video_path\n",
    "        ]\n",
    "        try:\n",
    "            duration = subprocess.check_output(ffmpeg_command, stderr=subprocess.STDOUT).decode().strip()\n",
    "            return float(duration)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            error_message = f\"Error getting video duration: {e.output.decode()}\"\n",
    "            print(error_message)\n",
    "            log_error(error_message)\n",
    "            return None\n",
    "\n",
    "    # Get the video duration\n",
    "    video_duration = get_video_duration(video)\n",
    "    if video_duration is None:\n",
    "        raise RuntimeError(\"Failed to retrieve video duration.\")\n",
    "\n",
    "    # Get the base timestamp to normalize the timeline\n",
    "    base_timestamp = world_timestamps['timestamp [ns]'].iloc[0]\n",
    "    # print(f\"Base timestamp: {base_timestamp}\")\n",
    "\n",
    "    # Process each ad \n",
    "    for index, row in events_df.iterrows():\n",
    "        event_id = row['event_id']\n",
    "\n",
    "        ad_save_dir = os.path.join(save_dir, f'{int(event_id)}')\n",
    "        os.makedirs(ad_save_dir, exist_ok=True)\n",
    "\n",
    "        nearest_timestamp_ns = row['nearest_timestamp_ns']\n",
    "        duration = row['duration_ns']\n",
    "\n",
    "        # Calculate the second timestamp\n",
    "        second_timestamp_ns = nearest_timestamp_ns + duration\n",
    "\n",
    "         # Add 0.5 seconds (500 million nanoseconds) before and after the event\n",
    "        buffer_ns = 500_000_000\n",
    "        start_timestamp_ns = nearest_timestamp_ns - buffer_ns\n",
    "        end_timestamp_ns = second_timestamp_ns + buffer_ns\n",
    "\n",
    "        # Find the frame numbers for both timestamps\n",
    "        frame_num_1 = np.searchsorted(world_timestamps['timestamp [ns]'], nearest_timestamp_ns)\n",
    "        frame_num_2 = np.searchsorted(world_timestamps['timestamp [ns]'], second_timestamp_ns)\n",
    "\n",
    "        frame_num_1 = int(frame_num_1)\n",
    "        frame_num_2 = int(frame_num_2)\n",
    "\n",
    "        # Get the start and end timestamps for the ad in nanoseconds\n",
    "        start_ts = world_timestamps['timestamp [ns]'].iloc[frame_num_1]\n",
    "        end_ts = world_timestamps['timestamp [ns]'].iloc[frame_num_2]\n",
    "\n",
    "        # Sort the gaze data by filtering the start and end timestamps\n",
    "        gaze_data = gaze[(gaze['timestamp [ns]'] >= start_ts) & (gaze['timestamp [ns]'] <= end_ts)]\n",
    "        gaze_data.to_csv(os.path.join(ad_save_dir, f'gaze.csv'), index=False)\n",
    "\n",
    "        # Normalize timestamps to video timeline\n",
    "        normalized_start_ts = start_ts - base_timestamp\n",
    "        normalized_end_ts = end_ts - base_timestamp\n",
    "\n",
    "        # Find the corresponding frame numbers\n",
    "        start_index = np.searchsorted(world_timestamps['timestamp [ns]'], normalized_start_ts + base_timestamp)\n",
    "        end_index = np.searchsorted(world_timestamps['timestamp [ns]'], normalized_end_ts + base_timestamp)\n",
    "\n",
    "        # Convert normalized timestamps to time in seconds\n",
    "        start_time = normalized_start_ts / 1e9\n",
    "        end_time = normalized_end_ts / 1e9\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # Check if the duration matches the duration of the gaze_data\n",
    "        gaze_duration = (gaze_data['timestamp [ns]'].iloc[-1] - gaze_data['timestamp [ns]'].iloc[0]) / 1e9\n",
    "        if abs(duration - gaze_duration) > 0.01:\n",
    "            print(f\"Warning: Duration mismatch between video segment and gaze data. Video duration: {duration}, Gaze data duration: {gaze_duration}\")\n",
    "\n",
    "        # Debug: Print calculated times\n",
    "        # print(f\"Event {int(event_id)}: {start_ts} to {end_ts}\")\n",
    "        # print(f\"Start Time: {start_time}, End Time: {end_time}, Duration: {duration}\")\n",
    "\n",
    "        # Check if the start time and duration are valid\n",
    "        if start_time < 0 or duration <= 0 or start_time + duration > video_duration:\n",
    "            print(f\"Invalid time range for {start_ts} to {end_ts}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Output file path\n",
    "        output_path = os.path.join(ad_save_dir, f'{int(event_id)}.mp4')\n",
    "\n",
    "        # FFmpeg command to extract the video segment\n",
    "        ffmpeg_command = [\n",
    "            'ffmpeg',\n",
    "            '-i', video,\n",
    "            '-ss', f'{start_time:.3f}',  # Start time with millisecond precision\n",
    "            '-t', f'{duration:.3f}',     # Duration with millisecond precision\n",
    "            '-c:v', 'libx264',           # Video codec\n",
    "            '-c:a', 'aac',               # Audio codec\n",
    "            '-strict', 'experimental',\n",
    "            '-y',                        # Overwrite output file\n",
    "            output_path\n",
    "        ]\n",
    "\n",
    "        # Execute the FFmpeg command\n",
    "        try:\n",
    "            subprocess.run(ffmpeg_command, check=True, stderr=subprocess.PIPE)\n",
    "            print(f\"Segment saved: {output_path}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            error_message = f\"Error extracting segment {start_ts} to {end_ts}: {e.stderr.decode()}\"\n",
    "            print(error_message)\n",
    "            log_error(error_message)\n",
    "\n",
    "    print(\"Video processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-098_phone-b-29fbdee7/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-098_phone-b-29fbdee7/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-098_phone-b-29fbdee7/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-098_phone-b-29fbdee7/ads/4/4.mp4\n",
      "Video processing completed.\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "process_video_segments_phone(video, world_timestamps_df, aligned, gaze, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Telvision AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt 8 # would be cool to have a print out of what % complete it is...idk about that rn (not round here partner, not round here)\n",
    "def detect_and_save_smartphones(video_path, output_folder, skip_frames=5):\n",
    "    net = cv2.dnn.readNet(\"/Users/ebeard/Documents/darknet/yolov3.weights\", \"/Users/ebeard/Documents/darknet/cfg/yolov3.cfg\")\n",
    "    layer_names = net.getLayerNames()\n",
    "    classes = []\n",
    "    with open(\"/Users/ebeard/Documents/darknet/data/coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # Handling output layers based on the format of getUnconnectedOutLayers\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    if unconnected_out_layers.ndim == 1:\n",
    "        output_layers = [layer_names[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "    else:\n",
    "        output_layers = [layer_names[i[0] - 1] for i in unconnected_out_layers]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_index = 0\n",
    "    aoi_data = []\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_folder, \"frames\")):\n",
    "        os.makedirs(os.path.join(output_folder, \"frames\"))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if skip_frames == 0 or (frame_index % skip_frames == 0):  # Process this frame\n",
    "            height, width, channels = frame.shape\n",
    "            blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "            net.setInput(blob)\n",
    "            outs = net.forward(output_layers)\n",
    "\n",
    "            class_ids = []\n",
    "            confidences = []\n",
    "            boxes = []\n",
    "\n",
    "            for out in outs:\n",
    "                for detection in out:\n",
    "                    scores = detection[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "                    if confidence > 0.5:\n",
    "                        center_x = int(detection[0] * width)\n",
    "                        center_y = int(detection[1] * height)\n",
    "                        w = int(detection[2] * width)\n",
    "                        h = int(detection[3] * height)\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        if classes[class_id].lower() == 'cell phone': \n",
    "                            boxes.append([x, y, w, h])\n",
    "                            confidences.append(float(confidence))\n",
    "                            class_ids.append(class_id)\n",
    "\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "            found_phone = False\n",
    "            largest_box = None\n",
    "            max_area = 0\n",
    "            \n",
    "            if len(indexes) > 0:  # Check if indexes is not empty\n",
    "                for i in indexes.flatten():\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    area = w * h\n",
    "                    if area > max_area:  # Find the largest bounding box\n",
    "                        largest_box = (x, y, w, h)\n",
    "                        max_area = area\n",
    "\n",
    "                if largest_box is not None:\n",
    "                    x, y, w, h = largest_box\n",
    "                    label = str(classes[class_ids[i]])\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    aoi_data.append({'frame': frame_index, 'x': x, 'y': y, 'width': w, 'height': h})\n",
    "                    found_phone = True\n",
    "                    \n",
    "            if found_phone:\n",
    "                cv2.imwrite(os.path.join(output_folder, \"frames\", f\"frame_{frame_index}.jpg\"), frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    for i in range(2):\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # Save rectangle data to a CSV file\n",
    "    with open(os.path.join(output_folder, 'rectangle_data.csv'), 'w', newline='') as csvfile: \n",
    "        fieldnames = ['frame', 'x', 'y', 'width', 'height']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for data in aoi_data:\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Example usage\n",
    "#aois = detect_and_save_smartphones(video, 'full_frame', skip_frames=100)\n",
    "#television = detect_and_save_smartphones(video_path = os.path.join('data', 'sub-019_tv-lego-42b0cb7d', '09e25ad2_0.0-784.762.mp4'), output_folder='full_frame_tv', skip_frames=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "detect_and_save_smartphones(ad_video, ad_save_dir, skip_frames=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AOI for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_aoi_data_from_video(rectangle_df, video_path, save_dir):\n",
    "    \"\"\"\n",
    "    Interpolates missing AOI data (x, y, width, height) based on the number of frames in the video.\n",
    "    \n",
    "    Parameters:\n",
    "    - rectangle_df (pd.DataFrame): A dataframe containing 'frame', 'x', 'y', 'width', 'height' columns for detected AOIs.\n",
    "    - video_path (str): Path to the video file to get the total number of frames.\n",
    "    \n",
    "    Returns:\n",
    "    - interpolated_df (pd.DataFrame): A dataframe with interpolated AOI values for each frame in the video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if rectangle_df is a file path\n",
    "    if isinstance(rectangle_df, str):\n",
    "        rectangle_df = pd.read_csv(rectangle_df)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Get total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()  # Close the video capture\n",
    "\n",
    "    # Ensure rectangle_df contains only necessary columns and is sorted by frame\n",
    "    # Calculate the area of each rectangle\n",
    "    rectangle_df['area'] = rectangle_df['width'] * rectangle_df['height']\n",
    "    \n",
    "    # Calculate the IQR for area\n",
    "    Q1 = rectangle_df['area'].quantile(0.25)\n",
    "    Q3 = rectangle_df['area'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter out outliers based on the IQR method\n",
    "    filtered_df = rectangle_df[(rectangle_df['area'] >= lower_bound) & (rectangle_df['area'] <= upper_bound)]\n",
    "\n",
    "    filtered_df = filtered_df[['frame', 'x', 'y', 'width', 'height']].sort_values(by='frame').reset_index(drop=True)\n",
    "    \n",
    "    # Create a DataFrame that has all frame numbers from 0 to total_frames - 1\n",
    "    all_frames = pd.DataFrame({'frame': np.arange(total_frames)})\n",
    "    \n",
    "    # Merge with the existing AOI data (this will insert NaNs for missing AOI data)\n",
    "    full_frame_df = pd.merge(all_frames, filtered_df, how='left', on='frame')\n",
    "    \n",
    "    # Interpolate missing values for x, y, width, height\n",
    "    full_frame_df[['x', 'y', 'width', 'height']] = full_frame_df[['x', 'y', 'width', 'height']].interpolate(\n",
    "        method='linear', limit_direction='forward', axis=0)\n",
    "    \n",
    "    # Fill any remaining NaNs with the nearest available values\n",
    "    full_frame_df = full_frame_df.bfill()\n",
    "    full_frame_df = full_frame_df.ffill()\n",
    "\n",
    "    # Plot the distribution of the areas and add the lower bound line\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(rectangle_df['area'], bins=20, color='blue', edgecolor='black', label='AOI Areas')\n",
    "\n",
    "    # Add vertical lines for the quartiles and IQR bounds\n",
    "    plt.axvline(rectangle_df['area'].quantile(0.25), color='red', linestyle='dashed', linewidth=1.5, label=\"25th Percentile\")\n",
    "    plt.axvline(rectangle_df['area'].quantile(0.50), color='green', linestyle='dashed', linewidth=1.5, label=\"50th Percentile\")\n",
    "    plt.axvline(rectangle_df['area'].quantile(0.75), color='orange', linestyle='dashed', linewidth=1.5, label=\"75th Percentile\")\n",
    "    plt.axvline(lower_bound, color='purple', linestyle='dashed', linewidth=1.5, label=\"Lower Bound (Outliers)\")\n",
    "\n",
    "    plt.title('Distribution of AOI Areas with Lower Bound Line')\n",
    "    plt.xlabel('Area (width * height)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plot_path = os.path.join(save_dir, 'distribution_of_aois.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    return full_frame_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a filtered AOI dataframe 'filtered_rectangle_df' and video 'input_video.mp4'\n",
    "# interpolated_aoi_df = interpolate_aoi_data_from_video(filtered_rectangle_df, 'input_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "rectangle_df = pd.read_csv(os.path.join(save_dir, '6', 'rectangle_data.csv'))\n",
    "# aoi_df = interpolate_aoi_data_from_video(rectangle_df, ad_video)\n",
    "# Get the filtered data and lower bound\n",
    "filtered_aoi_df = interpolate_aoi_data_from_video(rectangle_df, os.path.join(save_dir, '6', '6.mp4'), os.path.join(save_dir, '6'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Gaze Onto Scene Video, Gaze Within AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fixation_durations(fixation_data):\n",
    "    \"\"\" Calculate fixation duration based on the number of gaze points for each fixation ID. \"\"\"\n",
    "    fixation_data = fixation_data.copy()\n",
    "    fixation_data.loc[:,'fixation_duration'] = fixation_data.groupby('fixation id')['fixation id'].transform('count')\n",
    "    return fixation_data\n",
    "\n",
    "def is_point_in_aoi(gaze_x, gaze_y, aoi_row):\n",
    "    \"\"\" Check if a given gaze point (gaze_x, gaze_y) is inside the AOI rectangle. \"\"\"\n",
    "    aoi_x = int(aoi_row['x'])\n",
    "    aoi_y = int(aoi_row['y'])\n",
    "    aoi_width = int(aoi_row['width'])\n",
    "    aoi_height = int(aoi_row['height'])\n",
    "    \n",
    "    return aoi_x <= gaze_x <= aoi_x + aoi_width and aoi_y <= gaze_y <= aoi_y + aoi_height\n",
    "\n",
    "def add_within_aoi_column(gaze_data, aoi_data, video_start_timestamp, frame_duration_ns):\n",
    "    \"\"\" Add a 'within_aoi' column to the gaze data, indicating if the gaze point is within the AOI. \"\"\"\n",
    "    within_aoi = []\n",
    "    \n",
    "    for _, gaze_row in gaze_data.iterrows():\n",
    "        gaze_timestamp = gaze_row['timestamp [ns]']\n",
    "        gaze_x = int(gaze_row['gaze x [px]'])\n",
    "        gaze_y = int(gaze_row['gaze y [px]'])\n",
    "        \n",
    "        # Find the corresponding frame based on the timestamp\n",
    "        frame_idx = int((gaze_timestamp - video_start_timestamp) // frame_duration_ns)\n",
    "        aoi_row = aoi_data[aoi_data['frame'] == frame_idx]\n",
    "        \n",
    "        if not aoi_row.empty:\n",
    "            # Check if the gaze point is within the AOI\n",
    "            within_aoi.append(is_point_in_aoi(gaze_x, gaze_y, aoi_row.iloc[0]))\n",
    "        else:\n",
    "            within_aoi.append(False)  # If no AOI for the frame, assume False\n",
    "    \n",
    "    gaze_data['within_aoi'] = within_aoi\n",
    "    return gaze_data\n",
    "\n",
    "def overlay_gaze_and_aoi_on_video_ffmpeg(video_path, gaze_data, aoi_data, output_path):\n",
    "    \"\"\"\n",
    "    Overlays averaged gaze points (both fixation and non-fixation) onto each frame of a video.\n",
    "    Scanpaths are drawn between averaged gaze points for continuous movement visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path (str): Path to the input video file.\n",
    "    - gaze_data (pd.DataFrame): Raw gaze data with 'timestamp [ns]', 'gaze x [px]', 'gaze y [px]', 'fixation id' columns.\n",
    "    - aoi_data (pd.DataFrame): AOI data with 'frame', 'x', 'y', 'width', 'height' columns for each frame.\n",
    "    - output_path (str): Path to save the output video with AOIs, gaze points, and scanpaths overlaid.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if gaze_data is a file path\n",
    "    if isinstance(gaze_data, str):\n",
    "        gaze_data = pd.read_csv(gaze_data)\n",
    "\n",
    "    # Check if aoi_data is a file path\n",
    "    if isinstance(aoi_data, str):\n",
    "        aoi_data = pd.read_csv(aoi_data)\n",
    "\n",
    "    # Calculate fixation durations based on the number of gaze points per fixation ID\n",
    "    fixation_data = gaze_data[gaze_data['fixation id'].notna()]\n",
    "    fixation_data = calculate_fixation_durations(fixation_data)  # Calculate fixation durations\n",
    "    \n",
    "    # Calculate the frame duration based on average_rate\n",
    "    input_container = av.open(video_path)\n",
    "    input_stream = input_container.streams.video[0]\n",
    "    frame_rate = input_stream.average_rate\n",
    "    frame_duration_ns = int(1 / frame_rate * 1e9)\n",
    "    \n",
    "    # Find the start timestamp of the video\n",
    "    video_start_timestamp = gaze_data['timestamp [ns]'].min()\n",
    "\n",
    "    # Add the 'within_aoi' column to the gaze data\n",
    "    gaze_data = add_within_aoi_column(gaze_data, aoi_data, video_start_timestamp, frame_duration_ns)\n",
    "\n",
    "    # Define the path for the output video\n",
    "    new_video_path = os.path.join(output_path, 'output_with_aoi_and_gaze.mp4')\n",
    "    \n",
    "    # Open the video file using PyAV\n",
    "    output_container = av.open(new_video_path, mode='w')\n",
    "    output_stream = output_container.add_stream('mpeg4', rate=input_stream.average_rate)\n",
    "    output_stream.width = input_stream.width\n",
    "    output_stream.height = input_stream.height\n",
    "    output_stream.pix_fmt = 'yuv420p'\n",
    "\n",
    "    # Variables to store previous averaged gaze point for scanpath drawing\n",
    "    prev_gaze_x = None\n",
    "    prev_gaze_y = None\n",
    "    \n",
    "    # Iterate over frames in the input video\n",
    "    for frame_idx, frame in enumerate(input_container.decode(video=0)):\n",
    "        # Convert the PyAV frame to a PIL image for drawing and to to RGB explicitly (to avoid the warning)\n",
    "        img = frame.to_image().convert(\"RGB\")\n",
    "\n",
    "        # Calculate the timestamp for the current frame\n",
    "        current_frame_timestamp = video_start_timestamp + frame_idx * frame_duration_ns\n",
    "\n",
    "        # Draw AOI for the current frame if available\n",
    "        aoi_row = aoi_data[aoi_data['frame'] == frame_idx]\n",
    "        if not aoi_row.empty:\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            aoi_x = int(aoi_row['x'].values[0])\n",
    "            aoi_y = int(aoi_row['y'].values[0])\n",
    "            aoi_width = int(aoi_row['width'].values[0])\n",
    "            aoi_height = int(aoi_row['height'].values[0])\n",
    "            draw.rectangle([(aoi_x, aoi_y), (aoi_x + aoi_width, aoi_y + aoi_height)], outline=(0, 255, 0), width=4)\n",
    "\n",
    "        # Find gaze points for the current frame and calculate the average\n",
    "        gaze_points = gaze_data[(gaze_data['timestamp [ns]'] >= current_frame_timestamp) &\n",
    "                                (gaze_data['timestamp [ns]'] < current_frame_timestamp + frame_duration_ns)]\n",
    "        \n",
    "        if not gaze_points.empty:\n",
    "            # Average the gaze points for this frame\n",
    "            avg_gaze_x = int(gaze_points['gaze x [px]'].mean())\n",
    "            avg_gaze_y = int(gaze_points['gaze y [px]'].mean())\n",
    "            is_within_aoi = gaze_points['within_aoi'].mean() > 0.5  # If majority of points are inside AOI\n",
    "            \n",
    "            # Draw gaze point based on whether it's within AOI\n",
    "            color = \"blue\" if is_within_aoi else \"orange\"\n",
    "            draw.ellipse([(avg_gaze_x - 3, avg_gaze_y - 3), (avg_gaze_x + 3, avg_gaze_y + 3)], fill=color, outline=color)\n",
    "            \n",
    "            # Draw scanpath (line between consecutive averaged gaze points)\n",
    "            if prev_gaze_x is not None and prev_gaze_y is not None:\n",
    "                draw.line([(prev_gaze_x, prev_gaze_y), (avg_gaze_x, avg_gaze_y)], fill=\"blue\", width=2)\n",
    "\n",
    "            # Update the previous gaze point for the next frame\n",
    "            prev_gaze_x, prev_gaze_y = avg_gaze_x, avg_gaze_y\n",
    "\n",
    "        # Find fixation points within the current frame's time window and calculate the average\n",
    "        fixation_rows = fixation_data[(fixation_data['timestamp [ns]'] >= current_frame_timestamp) &\n",
    "                                      (fixation_data['timestamp [ns]'] < current_frame_timestamp + frame_duration_ns)]\n",
    "        if not fixation_rows.empty:\n",
    "            # Average the fixation points for this frame\n",
    "            avg_fixation_x = int(fixation_rows['gaze x [px]'].mean())\n",
    "            avg_fixation_y = int(fixation_rows['gaze y [px]'].mean())\n",
    "            avg_fixation_duration = fixation_rows['fixation_duration'].mean()\n",
    "            \n",
    "            # Base size of the circle and scaling it by the averaged fixation duration\n",
    "            base_size = 3\n",
    "            scaled_size = base_size + 2 * (avg_fixation_duration ** 0.5)\n",
    "            radius = int(scaled_size)\n",
    "            \n",
    "            # Draw the averaged fixation point with no fill and size proportional to the averaged fixation duration\n",
    "            draw.ellipse([(avg_fixation_x - radius, avg_fixation_y - radius), (avg_fixation_x + radius, avg_fixation_y + radius)], outline=\"blue\", width=2)\n",
    "\n",
    "        # Convert the image back to a PyAV frame\n",
    "        frame = av.VideoFrame.from_image(img)\n",
    "\n",
    "        # Encode and write the frame\n",
    "        packet = output_stream.encode(frame)\n",
    "        output_container.mux(packet)\n",
    "    \n",
    "    # Finalize the video\n",
    "    packet = output_stream.encode(None)\n",
    "    output_container.mux(packet)\n",
    "\n",
    "    # Close the input and output containers\n",
    "    input_container.close()\n",
    "    output_container.close()\n",
    "\n",
    "    print(f\"Output video with averaged gaze points, fixations, and scanpath saved as {new_video_path}\")\n",
    "\n",
    "     # print the updated gaze_data with 'within_aoi' column\n",
    "    # Save the updated gaze data to a new CSV file\n",
    "    gaze_data.to_csv(os.path.join(output_path, 'updated_gaze_data.csv'), index=False)\n",
    "\n",
    "# Example usage:\n",
    "# video_start_timestamp = 1712761742852933376  # Taken from the 'aligned_events.csv'\n",
    "# overlay_gaze_and_aoi_on_video_ffmpeg('input_video.mp4', gaze_data_raw_df, aoi_df, 'output_with_averaged_gaze_and_scanpath.mp4', video_start_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "video_gaze_data = os.path.join(save_dir, '7', 'gaze.csv')\n",
    "overlay_gaze_and_aoi_on_video_ffmpeg(ad_video, video_gaze_data, filtered_aoi_df, os.path.join(save_dir, '7'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Through Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Starting processing for  sub-095  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-096  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-097  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-098  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-099  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-100  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-104  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-105  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-106  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-107  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-109  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-110  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-111  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-115  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-116  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-117  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-119  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-158  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-161  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-161_phone-a-49a6f541/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 17851/535000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "------------ Starting processing for  sub-162  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-163  ------------\n",
      "Skipping subject as output already exists.\n",
      "------------ Starting processing for  sub-164  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/4/4.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 143 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 184 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 140 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 144 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-164_phone-b-0630a2dc/ads/4/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-168  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 143 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 167 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 164 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 128 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 645 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 43 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 156 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 48 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-168_phone-a-93b8510d/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-170  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 145 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 154 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 138 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 140 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 13 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 35 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 11 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 17 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-170_phone-a-d58d2945/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-171  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/4/4.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 54 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 50 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 33 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 59 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-171_phone-b-fb-aaf8a03e/ads/4/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-171  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-yt-2da75edf/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-yt-2da75edf/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-171_phone-b-yt-2da75edf/ads/3/3.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 14044/419625 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "------------ Starting processing for  sub-177  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 16 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 29 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 50 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 115 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 119 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 117 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 128 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-177_phone-b-00e10d49/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-178  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 145 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 141 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 136 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 145 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 68 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 68 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 31 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 65 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-178_phone-a-94fa808e/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-179  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 56 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 8 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 15 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 5 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 118 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 126 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 115 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 69 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-179_phone-b-d386e290/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-183  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-183_phone-a-d0b59b61/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 540271/16188500 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "------------ Starting processing for  sub-184  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 111 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 23 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 58 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 13 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 140 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 114 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 135 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 128 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-184_phone-b-60f84e14/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-185  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-185_phone-a-f3cd824e/ads/7/7.mp4\n",
      "Error processing video segments: single positional indexer is out-of-bounds\n",
      "------------ Starting processing for  sub-186  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/6/6.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 651 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 155 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 493 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 134 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 129 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 124 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-186_phone-b-4a061477/ads/6/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-187  ------------\n",
      "Splitting ads... \n",
      "Error aligning events: single positional indexer is out-of-bounds\n",
      "------------ Starting processing for  sub-188  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 41 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 35 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 571 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 96 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 147 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 143 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 167 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 152 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-188_phone-b-517bf78f/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-189  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 182 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/1/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 183 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/2/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 143 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/3/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 152 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/4/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 24 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/5/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 16 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/6/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 607 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/7/output_with_aoi_and_gaze.mp4\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      " (repeated 78 more times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video with averaged gaze points, fixations, and scanpath saved as /Volumes/T7 Shield/fox/et/sub-189_phone-a-231b32e7/ads/8/output_with_aoi_and_gaze.mp4\n",
      "------------ Starting processing for  sub-190  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/7/7.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-190_phone-b-3aedd9d6/ads/8/8.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 646389/19366000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "------------ Starting processing for  sub-191  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/3/3.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/4/4.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/5/5.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/6/6.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-191_phone-a-7a08ac42/ads/7/7.mp4\n",
      "Video processing completed.\n",
      "Processing ad  1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "Processing ad  7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error overlaying gaze to video: [Errno 22] Invalid argument; last error log: [mpeg4] timebase 964057/28900000 not supported by MPEG 4 standard, the maximum admitted value for the timebase denominator is 65535\n",
      "------------ Starting processing for  sub-192  ------------\n",
      "Splitting ads... \n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-192_phone-b-8962a1bf/ads/1/1.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-192_phone-b-8962a1bf/ads/2/2.mp4\n",
      "Segment saved: /Volumes/T7 Shield/fox/et/sub-192_phone-b-8962a1bf/ads/3/3.mp4\n",
      "Error processing video segments: single positional indexer is out-of-bounds\n"
     ]
    }
   ],
   "source": [
    "for subj in subj_folders[:]:\n",
    "\n",
    "    try:\n",
    "        subj_id = subj.split('/')[5].split('_')[0]\n",
    "        print('------------ Starting processing for ', subj_id,' ------------')\n",
    "        \n",
    "        # subj info and files\n",
    "        subj_folder = subj\n",
    "\n",
    "        try:\n",
    "            video = glob.glob(os.path.join(subj, '*.mp4'))[0]\n",
    "        except IndexError as e:\n",
    "            error_message = f\"Video file not found: {e}\"\n",
    "            print(error_message)\n",
    "            log_error(error_message, subj=subj)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            events_df = pd.read_csv(os.path.join(subj_folder, 'events.csv'))\n",
    "            world_timestamps_df = pd.read_csv(os.path.join(subj_folder, 'world_timestamps.csv'))\n",
    "            gaze = pd.read_csv(os.path.join(subj_folder, 'gaze.csv'))\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            error_message = f\"CSV file not found: {e}\"\n",
    "            print(error_message)\n",
    "            log_error(error_message, subj=subj_id)\n",
    "            continue\n",
    "\n",
    "        # if we've run this before, skip it\n",
    "        skip_test = glob.glob(os.path.join(subj, 'ads', '[0-9]', 'output_with_aoi_and_gaze.mp4')) #'[0-9]*'))\n",
    "        \n",
    "        if skip_test:\n",
    "            print(f\"Skipping subject as output already exists.\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            print('Splitting ads... ')\n",
    "\n",
    "            save_dir = os.path.join(subj, 'ads')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # align pupil labs with openbci data\n",
    "            try:\n",
    "                aligned = create_event_onsets_and_durations(events_df, world_timestamps_df)\n",
    "                aligned.to_csv(os.path.join(save_dir, 'aligned_events.csv'), index=False)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_message = f\"Error aligning events: {e}\"\n",
    "                print(error_message)\n",
    "                log_error(error_message, subj=subj_id)\n",
    "                continue\n",
    "\n",
    "            # save open qc plots\n",
    "            try:\n",
    "                plot_adjusted_onsets_vs_world_timestamps(aligned, world_timestamps_df, save_dir)\n",
    "                plot_nearest_timestamps(aligned, save_dir)\n",
    "                plot_event_durations(aligned, save_dir)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_message = f\"Error generating plots: {e}\"\n",
    "                print(error_message)\n",
    "                log_error(error_message, subj=subj_id)\n",
    "                continue\n",
    "\n",
    "            # extract pupil labs videos\n",
    "            try:\n",
    "                process_video_segments_phone(video, world_timestamps_df, aligned, gaze, save_dir)\n",
    "            except Exception as e:\n",
    "                error_message = f\"Error processing video segments: {e}\"\n",
    "                print(error_message)\n",
    "                log_error(error_message, subj=subj_id)\n",
    "                continue\n",
    "\n",
    "            # extract telvision AOI\n",
    "            ads_folders = glob.glob(os.path.join(subj, 'ads', '[0-9]*')) #'[0-9]*'))\n",
    "\n",
    "            for ad_folder in ads_folders:\n",
    "                short_video = glob.glob(os.path.join(ad_folder, '*.mp4'))[0]\n",
    "                event_id = short_video.split('/')[-1].split('.')[0]\n",
    "\n",
    "                print('Processing ad ', event_id,'...')\n",
    "\n",
    "                # make output folder\n",
    "                ad_save_dir = os.path.join(subj, 'ads', event_id)\n",
    "                os.makedirs(ad_save_dir, exist_ok=True)\n",
    "                \n",
    "                # detect tv screens\n",
    "                try:\n",
    "                    # if gaze data is longer than 7 seconds, skip frames = 5\n",
    "                    if aligned[aligned['event_id'] == int(event_id)]['duration_ns'].values[0] > 7e9:\n",
    "                        skip_frames = 5\n",
    "                    else:\n",
    "                        skip_frames = 0\n",
    "                        \n",
    "                    detect_and_save_smartphones(short_video, ad_save_dir, skip_frames=0)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_message = f\"Detecting tv screens: {e}\"\n",
    "                    print(error_message)\n",
    "                    log_error(error_message, subj=short_video)\n",
    "                    continue\n",
    "\n",
    "                # filter rectangles\n",
    "                try:\n",
    "\n",
    "                    rectangle_df = os.path.join(ad_folder, 'rectangle_data.csv')\n",
    "                    aoi_df = interpolate_aoi_data_from_video(rectangle_df, short_video, ad_save_dir)\n",
    "                    aoi_df.to_csv(os.path.join(ad_folder, 'processed_aoi_for_each_frame.csv'))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error filtering rectangles: {e}\"\n",
    "                    print(error_message)\n",
    "                    log_error(error_message, subj=subj_id)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    video_gaze_data = os.path.join(ad_folder, 'gaze.csv')\n",
    "                    overlay_gaze_and_aoi_on_video_ffmpeg(short_video, video_gaze_data, aoi_df, ad_save_dir)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error overlaying gaze to video: {e}\"\n",
    "                    print(error_message)\n",
    "                    log_error(error_message, subj=subj_id)\n",
    "                    continue\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Unexpected error: {e}\"\n",
    "        print(error_message)\n",
    "        log_error(error_message, subj=subj_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
